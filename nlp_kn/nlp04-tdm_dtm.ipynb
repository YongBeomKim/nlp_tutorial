{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **자연어 4일 - Embadding, Search Engine**\n",
    "- Document Representation : **[Source Code](http://bitly.kr/XKcjfe)**\n",
    "- **[스탠포드 공대 IR 수업자료](https://nlp.stanford.edu/IR-book/newslides.html)** \n",
    "- **[버지니아 공대 NLP 수업자료](http://www.cs.virginia.edu/~hw5x/Course/IR2015/_site/lectures/)**\n",
    "\n",
    "# **Preview**\n",
    "1. **Konlpy 의 모듈** 중, 명사구분/ 형용사 구분 등 **필요한 성능에 따라** 다양한 모듈의 적용\n",
    "1. Komoran 을 활용시 **java.lang.OutOfMemoryError: GC overhead limit exceeded** 오류시 주의할 것\n",
    "1. 때문에 여러가지 이유로 **Mecab** 을 추천 합니다.\n",
    "\n",
    "```python\n",
    "# java.lang.OutOfMemoryError 오류시 Java 메모리 할당 추가하기\n",
    "# 참고 사이트 : https://github.com/konlpy/konlpy/issues/93\n",
    "import os\n",
    "os.environ['JAVA_OPTS'] = '-Xmx4096M' # -Xmx1024M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EC': '연결 어미', 'EF': '종결 어미', 'EP': '선어말어미', 'ETM': '관형형 전성 어미', 'ETN': '명사형 전성 어미', 'IC': '감탄사', 'JC': '접속 조사', 'JKB': '부사격 조사', 'JKC': '보격 조사', 'JKG': '관형격 조사', 'JKO': '목적격 조사', 'JKQ': '인용격 조사', 'JKS': '주격 조사', 'JKV': '호격 조사', 'JX': '보조사', 'MAG': '일반 부사', 'MAJ': '접속 부사', 'MM': '관형사', 'NNB': '의존 명사', 'NNBC': '단위를 나타내는 명사', 'NNG': '일반 명사', 'NNP': '고유 명사', 'NP': '대명사', 'NR': '수사', 'SC': '구분자 , · / :', 'SE': '줄임표 …', 'SF': '마침표, 물음표, 느낌표', 'SH': '한자', 'SL': '외국어', 'SN': '숫자', 'SSC': '닫는 괄호 ), ]', 'SSO': '여는 괄호 (, [', 'SY': '기타 기호', 'VA': '형용사', 'VCN': '부정 지정사', 'VCP': '긍정 지정사', 'VV': '동사', 'VX': '보조 용언', 'XPN': '체언 접두사', 'XR': '어근', 'XSA': '형용사 파생 접미사', 'XSN': '명사파생 접미사', 'XSV': '동사 파생 접미사'}\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('다', 'EF'), ('.', 'SF')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어절 생성하는 함수 (N-Gram)\n",
    "def eojeol(sent, num=2): # num > 2\n",
    "    ngram = []\n",
    "    token = sent.split() # Token 의 생성 (각자 다르게 적용)\n",
    "    for i in range(len(token) - (num-1)):\n",
    "        ngram.append(tuple(token[i: i+num]))\n",
    "    return ngram\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "sentence = \"아버지가방에들어가다.\"\n",
    "print(Mecab().tagset)\n",
    "print(Mecab().pos(sentence))\n",
    "[_  for _ in Mecab().pos(sentence) if _[1] not in [\"EF\", \"SF\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word Token Embadding**\n",
    "## **1. 자료 불러오기**\n",
    "- Token String 을 **Index** 로 사용하면 자료가 커진다\n",
    "- **Token Index** 를 외래키로 사용하여 **DataBase** 크기를 줄인다\n",
    "\n",
    "```python\n",
    "tokens = word_tokenize(\"\\n\".join(collection))\n",
    "bigram = eojeol(\"\\n\".join(collection))\n",
    "pos    = Okt().pos(\"\\n\".join(collection))\n",
    "len(tokens + bigram + Voca), len(set(tokens + bigram + Voca))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Pages : 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66151, 29259)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식에서 숫자는 : String Range 값을 특정 합니다\n",
    "import os, re\n",
    "fileFolder = \"./News/\" \n",
    "fileList   = [fileFolder + _  for _ in os.listdir(fileFolder) \n",
    "               if re.match(\"\\d{10}.txt\", _) ]\n",
    "\n",
    "# 네이버 뉴스 수집자료를 collection 로 묶는다\n",
    "collection = []\n",
    "for _ in fileList:\n",
    "    with open(_) as f:\n",
    "        collection.append(f.read())\n",
    "\n",
    "# 문서 검색력을 높이기 위해 Token, Bigram, konlpy Tag 로 세분화 한다\n",
    "from konlpy.tag import Mecab\n",
    "from nltk.tokenize import word_tokenize\n",
    "Vocabulary = Mecab().nouns(\"\\n\".join(collection))\n",
    "bigram     = eojeol(\" \".join(Vocabulary))\n",
    "\n",
    "print(\"Collection Pages : {}\".format(len(collection)))\n",
    "len(bigram + Vocabulary), len(set(bigram + Vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Token 의 생성**\n",
    "- Token String 을 **Index** 로 사용하면 자료가 커진다\n",
    "- **Token Index** 를 외래키로 사용하여 **DataBase** 크기를 줄인다\n",
    "\n",
    "```python\n",
    "docRepresentation = {}\n",
    "for token in word_tokenize(collection[0]):\n",
    "    docRepresentation[Vocabulary.index(token)] = 1\n",
    "\n",
    "docRepresentation\n",
    "{114: 1, 9: 1, 92: 1, 111: 1, 22: 1, 72: 1 ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['일', '부상', '미사일', '전열', '결속', '독', '전투기', '실질', '분석', '방문', '추가', '무단', '대북', '북한', '중국']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token List를 바탕으로 Index 값 추출\n",
    "docIndex   = 20\n",
    "Vocabulary = list(set(Mecab().nouns(collection[docIndex])))\n",
    "print(Vocabulary[:15])\n",
    "Vocabulary.index(\"국방\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token 의 Index 로 변환된 자료형을 생성합니다\n",
    "docRepresentation = {}\n",
    "for token in Mecab().nouns(collection[docIndex]):\n",
    "    docRepresentation[Vocabulary.index(token)] = 1\n",
    "print(len(docRepresentation))\n",
    "docRepresentation.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = []\n",
    "for i in range(len(Vocabulary)):\n",
    "    if i in docRepresentation:\n",
    "        vector.append(docRepresentation[i])\n",
    "    else:\n",
    "        vector.append(0)\n",
    "vector[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Search Engine Index 만들기**\n",
    "**Word Token Indexing** , 인덱싱 자료를 활용한 **검색 Tree** 만들기\n",
    "1. **AD HOC 방식** : 고정된 데이터를 대상으로 **사용자의 요구에 따라** 쿼리가 변경\n",
    "2. **Filtering (추천시스템)** : 사용자의 요구는 동일하지만 **새로운 문서 및 문서의 가중치가 자동으로 변경**\n",
    "\n",
    "## **1 DTM 모델 만들기**\n",
    "**Word Token Indexing** 를 활용한 **Document Term Matrix** 만들기\n",
    "1. 장점은 쉽게 만들수 있다\n",
    "1. 단점으로는 **전체 Token을 확인하는 만큼** 검색 비용이 많이 든다\n",
    "\n",
    "```json\n",
    "DTM   word1  word2 ... => V\n",
    "doc1    0      1\n",
    "doc2    => D(Collection)\n",
    "O(|Q|*D*|D|)\n",
    "```\n",
    "\n",
    "```python\n",
    "[In:]  DTM\n",
    "[Out:] defaultdict(<function __main__.<lambda>()>,\n",
    "            {0: defaultdict(int, {'오류': 1, '우회': 1, '교수': 12, .....}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 문서 수: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 만으로는 표현에 한계가 존재한다\n",
    "from collections import defaultdict \n",
    "DTM = defaultdict(lambda:defaultdict(int))\n",
    "\n",
    "# 자료형 생성하기\n",
    "for i, doc in enumerate(collection):\n",
    "    for term in Mecab().nouns(doc):\n",
    "        DTM[i][term] += 1\n",
    "\n",
    "# Token 이 내용의 확인 : {doc1:{}}\n",
    "print(\"Collection 문서 수:\", len(DTM))\n",
    "DTM[0][\"교수\"]  # DTM[문서 인덱스][검색 Token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 TDM 모델 만들기**\n",
    "- DTM 모델을 활용하여 **Token 별 검색 Tree** 를 재구성 합니다\n",
    "- **엔트로피가 낮다** : 식별력이 높은 단어 (출현 빈도가 차이가 큰 모델링이 가능)\n",
    "- **[DTM/ TDM by scikit-learn ](https://medium.com/@omicro03/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-7%EC%9D%BC%EC%B0%A8-term-document-matrix-tdm-f959ce229ade)**\n",
    "\n",
    "```json\n",
    "TDM    doc1 doc2 ... => D(Collection)\n",
    "word1    0    1\n",
    "word2    1\n",
    "word3    => V\n",
    "|Q|*D, (Linked List) // DTM 과 비교해 간단 (단점은 미리 작업할 내용이 많아 유연한 운영엔 단점)\n",
    "word1, ptr => doc2, ptr => doc5, .. // \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오류', '우회', '함수', '추가', '교수', '점', '만점', '학생', '혁진', '코리아', '텍', '설문']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TDM : DTM 을 활용한 색인용 Token 사전을 메모리에서 관리 한다\n",
    "TDM = defaultdict(lambda:defaultdict(int))\n",
    "for doc, termDict in DTM.items():\n",
    "    for term, freq in termDict.items():\n",
    "        TDM[term][doc] = freq\n",
    "\n",
    "list(TDM.keys())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 36, 72, 88, 102], [1, 10, 20, 36, 50]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "query        = \"김정은 판문점\"\n",
    "searchResult = []\n",
    "\n",
    "for _ in word_tokenize(query):\n",
    "    docResult = [] # collection 문서별 Token 검색결과\n",
    "    for doc, termDict in DTM.items():\n",
    "        if _ in termDict.keys():\n",
    "            docResult.append(doc)\n",
    "    searchResult.append(docResult)\n",
    "\n",
    "# [ 김정은 검색결과,  판문점 검색결과 ]\n",
    "searchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 36]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# InterSection 작업결과 출력 (키워드 중복결과)\n",
    "# group = searchResult[0]\n",
    "# for _ in searchResult[1:]:\n",
    "#     group = set(group).intersection(_)\n",
    "\n",
    "group = [_ for _ in searchResult[0]  if _ in searchResult[1]]\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TMD 모델의 결과를 DTM 으로 확인하기\n",
    "# Query Token 이 Document 에 포함여부 확인\n",
    "[q in DTM[1]   for q in word_tokenize(query) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['오류', '우회', '함수', '추가', '교수', '점', '만점', '학생', '혁진', '코리아', '텍', '설문', '조사', '결과', '연구', '환경', '인식', '차이', '불', '실', '연합뉴스', '자료', '사진', '대전', '이재림', '기자', '국내', '청년', '과학자', '간', '차', '것', '진로', '관심', '여부', '간극', '파악', '일', '한국연구재단', '한국', '산업', '기술', '대학교', '재단', '정책', '혁신', '팀', '이해', '당사자', '심층', '진단', '내용', '이슈', '리포트', '발표', '월', '이공', '대학원', '박사', '후', '연구원', '천', '명', '상대', '분야', '지원', '사업', '수행', '책임자', '연구실', '문화', '수', '문항', '응답', '분석', '항목', '긍정', '경우', '비율', '부정', '압도', '수준', '기준', '표준', '값', '집단', '정도', '적극', '지도', '제자', '이상', '이', '지난해', '국제', '학술지', '네이처', '리더십', '문제', '공개', '외국', '사례', '상담', '동료', '선배', '차별', '과제', '참여', '경제', '보상', '집필진', '외', '비교', '때', '우리', '나라', '과학', '시사', '바', '노력', '필요', '지적', '웹', '네이버', '채널', '구독', '흥'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"한국연구재단\" in DTM[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 Inversed Index 검색모델 만들기**\n",
    "[Python 을 활용한 Posting 구조 실습하기](https://medium.com/@fro_g/writing-a-simple-inverted-index-in-python-3c8bcb52169a)\n",
    "- in-Memory 인덱스 : **Voca Token or Lexicon** (단어 => 어느문서? 위치 포인터)\n",
    "- **On-Disk :** 문서의 몇번째 정보로 저장 (**Post = on-dick** 데이터로 세분화)\n",
    "- 즉 **Token** 마다 **검색용 Token Tree** 를 미리 생성 및 활용 **(검색 비용의 최소화** 및 **키워드 관리**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 36, 72, 88, 102], [1, 10, 20, 36, 50]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색 query 내용을 포함하는 문장들 찾기\n",
    "query = \"김정은 판문점\"\n",
    "\n",
    "searchResult = []\n",
    "for q in word_tokenize(query):\n",
    "    searchResult.append(list(TDM[q].keys()))\n",
    "searchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5431, 5430)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabluary or Lexicon = in-memory 에 저장(단어 => 어느문서? 위치(포인터))\n",
    "# Posting = on-disk : Huge Data (어느문서?, )\n",
    "Vocabulary       = [] # TDM 저장된 단어목록\n",
    "globalVocabulary = {} # in-Memory : 단어와 위치값\n",
    "globalPosting    = [] # on-Dist : 디스크 문서저장\n",
    "\n",
    "for i, doc in enumerate(collection):\n",
    "    localTDM = defaultdict(int) # 문서별 TDM 계산객체\n",
    "    \n",
    "    # 문서별 단어의 빈도를 기록한다.\n",
    "    for term in Mecab().nouns(doc):\n",
    "        localTDM[term] += 1\n",
    "        \n",
    "    for term, freq in localTDM.items():\n",
    "        if term not in Vocabulary:\n",
    "            Vocabulary.append(term)\n",
    "            \n",
    "        termIdx = Vocabulary.index(term)\n",
    "        \n",
    "        if termIdx not in globalVocabulary.keys():\n",
    "            nextPtr = -1 # 더이상 해당 Token 이 발견되지 않을 때\n",
    "        else:\n",
    "            nextPtr = globalVocabulary[termIdx] # \n",
    "        \n",
    "        _posting = [i, freq, nextPtr]        \n",
    "        globalVocabulary[termIdx] = len(globalPosting)\n",
    "        globalPosting.append(_posting)\n",
    "\n",
    "len(globalVocabulary), max(globalVocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 1, 6830]\n",
      "[36, 3, 3738]\n",
      "[20, 1, 2097]\n",
      "[10, 1, 139]\n",
      "[1, 2, -1]\n"
     ]
    }
   ],
   "source": [
    "# 검색 Query Token 을 활용한 Tree 내용 살펴보기\n",
    "query = \"판문점\"\n",
    "ptr   = globalVocabulary[Vocabulary.index(query)]\n",
    "while True:\n",
    "    if ptr < 0:\n",
    "        break\n",
    "    print(globalPosting[ptr])\n",
    "    ptr = globalPosting[ptr][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[102, 88, 72, 36, 1], [50, 36, 20, 10, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TDM 모델을 활용한 Query 검색엔진\n",
    "# 검색결과를 빠르게 출력 합니다.\n",
    "query = \"김정은 판문점\"\n",
    "\n",
    "searchResult = list()\n",
    "for q in word_tokenize(query):\n",
    "    ptr = globalVocabulary[Vocabulary.index(q)]\n",
    "    _qResult = list()\n",
    "    while True:\n",
    "        _posting = globalPosting[ptr]\n",
    "        _qResult.append(_posting[0])\n",
    "        if _posting[-1] == -1:\n",
    "            break\n",
    "        ptr = _posting[-1]\n",
    "    searchResult.append(_qResult)\n",
    "\n",
    "searchResult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
