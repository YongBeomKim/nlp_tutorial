{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 **Tokenization**\n",
    "Korean Tokenization with KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 Byte Pair Encoding**\n",
    "의미를 갖는 더 작은 Sub Word 로 구성 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '는', '인공지능', '의', '한', '줄기', '입니다']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 단어내 의미를 가는 단위로 분할 합니다\n",
    "text = \"자연어 처리는 인공지능의 한 줄기 입니다\"\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "Mecab().morphs(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# 5장 **유사성과 모호성**\n",
    "Korean Tokenization with KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 다의어 동의어 식별방법**\n",
    "1. 위키백과 Word2Vec 학습모델의 활용 **[word2vec PreTrained Model](https://github.com/facebookresearch/fastText/search?q=ko&unscoped_q=ko)**\n",
    "1. WordNet 사전의 활용 **[부산대학교 korlex](http://korlex.pusan.ac.kr/)**\n",
    "1. RNN 모델을 활용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Tf-IDF 다의어 동의어 식별방법**\n",
    "**참신러닝 Tf-idf 자료 활용하기 [Git blog](https://leechamin.tistory.com/141)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터 불러오기\n",
    "import json\n",
    "with open(\"../backup/torch_data.json\", \"r\") as f:\n",
    "    docs = f.read()\n",
    "docs = json.loads(docs)\n",
    "doc1 = docs['doc1']\n",
    "doc2 = docs['doc2']\n",
    "doc3 = docs['doc3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".     16\n",
       "는     15\n",
       "들     14\n",
       ",     10\n",
       "하     10\n",
       "      ..\n",
       "정상     1\n",
       "면      1\n",
       "이타     1\n",
       "데      1\n",
       "지능     1\n",
       "Length: 186, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 내 Token의 빈도수 계산\n",
    "import pandas as pd\n",
    "def get_term_frequency(document, word_dict=None):\n",
    "    if word_dict is None:\n",
    "        word_dict = {}\n",
    "    words = document.split()\n",
    "\n",
    "    for w in words:\n",
    "        word_dict[w] = 1 + (0 if word_dict.get(w) is None else word_dict[w])\n",
    "    return pd.Series(word_dict).sort_values(ascending=False)\n",
    "\n",
    "get_term_frequency(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "해서    2\n",
       "한     2\n",
       "기     2\n",
       "겠     2\n",
       "죠     2\n",
       "     ..\n",
       "데     1\n",
       "평균    1\n",
       "건     1\n",
       "자신    1\n",
       "풀     1\n",
       "Length: 311, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 token 이 몇개의 문서에서 출현하였는지를 Counting\n",
    "def get_document_frequency(documents):\n",
    "    dicts, df = [], {}\n",
    "    vocab = set([])\n",
    "    for d in documents:\n",
    "        tf    = get_term_frequency(d)\n",
    "        dicts += [tf]\n",
    "        vocab = vocab | set(tf.keys())\n",
    "    \n",
    "    for v in list(vocab):\n",
    "        df[v] = 0\n",
    "        for dict_d in dicts:\n",
    "            if dict_d.get(v) is not None:\n",
    "                df[v] += 1\n",
    "    return pd.Series(df).sort_values(ascending=False)\n",
    "\n",
    "get_document_frequency([doc1, doc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>남자</td>\n",
       "      <td>9</td>\n",
       "      <td>9.887511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.887511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>요인</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.591674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.591674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>심리학</td>\n",
       "      <td>5</td>\n",
       "      <td>5.493061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.493061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>었</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.394449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.394449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency      doc1      doc2      doc3       max\n",
       "0   남자          9  9.887511  0.000000  0.000000  9.887511\n",
       "1   요인          6  0.000000  6.591674  0.000000  6.591674\n",
       "2  심리학          5  5.493061  0.000000  0.000000  5.493061\n",
       "3    었          4  0.000000  0.000000  4.394449  4.394449\n",
       "4    제          4  0.000000  0.000000  4.394449  4.394449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문서의 token 별 tf-idf 계산하는 함수\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "def get_tfidf(docs):\n",
    "    vocab, tfs = {}, []\n",
    "    for d in docs:\n",
    "        vocab =  get_term_frequency(d, vocab)\n",
    "        tfs  += [get_term_frequency(d)]\n",
    "    df = get_document_frequency(docs)\n",
    "\n",
    "    stats = []\n",
    "    for word, freq in vocab.items():\n",
    "        tfidfs = []\n",
    "        for idx in range(len(docs)):\n",
    "            if tfs[idx].get(word) is not None:\n",
    "                tfidfs += [tfs[idx][word] * np.log(len(docs) / df[word])]\n",
    "            else: tfidfs += [0]\n",
    "        stats.append((word, freq, *tfidfs, max(tfidfs)))\n",
    "    \n",
    "    _col = ('word','frequency','doc1','doc2','doc3','max')\n",
    "    return pd.DataFrame(stats, columns=_col\n",
    "            ).sort_values('max', ascending=False).reset_index(drop=True)\n",
    "\n",
    "get_tfidf([doc1, doc2, doc3]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>는</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>을</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>하</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency  doc1  doc2  doc3\n",
       "0    는         47    15    14    18\n",
       "1    을         39     8    10    21\n",
       "2    .         36    16    10    10\n",
       "3    하         33    10     9    14\n",
       "4    이         32     8     8    16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문서의 tf 벡터 만들기 (tf-idf 와 비교활용이 가능)\n",
    "def get_tf(docs):\n",
    "    vocab, tfs = {}, []\n",
    "    for d in docs:\n",
    "        vocab = get_term_frequency(d, vocab)\n",
    "        tfs  += [get_term_frequency(d)]\n",
    "\n",
    "    stats = []\n",
    "    for word, freq in vocab.items():\n",
    "        tf_v = []\n",
    "        for idx in range(len(docs)):\n",
    "            if tfs[idx].get(word) is not None:\n",
    "                tf_v += [tfs[idx][word]]\n",
    "            else: tf_v += [0]\n",
    "        stats.append((word, freq, *tf_v))\n",
    "    _col = ('word','frequency','doc1','doc2','doc3')\n",
    "    return pd.DataFrame(stats, columns=_col).sort_values('frequency', ascending=False)\n",
    "\n",
    "get_tf([doc1, doc2, doc3]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 Context 윈도우로 동시출현 단어정보의 활용**\n",
    "https://github.com/juneoh/fastcampus-pytorch-nlp 에서 자료 다운받기\n",
    "1. Document 에서 **동시에 출현하는 단어를** 활용하는 방법으로 **N-Gram 에서 PMI** 수집과 비슷\n",
    "1. **windowing** 은 일정한 크기의 window 를 움직이며 내부의 Unit 정보를 수집합니다.\n",
    "1. **Context Window :** 정보를 수집할 때 사용되는 윈도우로 인접한 단어들의 빈도를 계산한 행렬\n",
    "1. 윈도우 크기를 Hyper Parametor 로 사용자가 특정을 해야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 714 ms, total: 20 s\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "웹    현재        1\n",
       "     TED       2\n",
       "     사이트     350\n",
       "사이트  TED       3\n",
       "     웹       365\n",
       "            ... \n",
       "산책   바퀴벌레      1\n",
       "말벌   시키        1\n",
       "대해   말벌        1\n",
       "기생충  안         1\n",
       "생각   기생충       1\n",
       "Length: 2706519, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "with open('../backup/ted.aligned.ko.refined.tok.txt') as f:\n",
    "    lines = [l.strip() for l in f.read().splitlines() if l.strip()]\n",
    "\n",
    "# context window 를 생성하는 함수\n",
    "def get_context_counts(lines, w_size=2):\n",
    "    co_dict   = defaultdict(int)\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        for i, w in enumerate(words):\n",
    "            for c in words[i - w_size:i + w_size]:\n",
    "                if w != c: co_dict[(w, c)] += 1\n",
    "    return pd.Series(co_dict)\n",
    "\n",
    "co_dict = get_context_counts(lines)\n",
    "co_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".      337685\n",
       "는      234692\n",
       "이      224422\n",
       "을      193583\n",
       "은      137351\n",
       "        ...  \n",
       "711         1\n",
       "967         1\n",
       "선거제         1\n",
       "아쉬웠         1\n",
       "끼친다         1\n",
       "Length: 62960, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = get_term_frequency(' '.join(lines))\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "고    98136\n",
       "것    78869\n",
       "한    63132\n",
       "죠    56568\n",
       "그    53895\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = tfs[tfs < 100000]\n",
    "tfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 91.9 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>고</th>\n",
       "      <th>것</th>\n",
       "      <th>한</th>\n",
       "      <th>죠</th>\n",
       "      <th>그</th>\n",
       "      <th>입니다</th>\n",
       "      <th>우리</th>\n",
       "      <th>수</th>\n",
       "      <th>에서</th>\n",
       "      <th>었</th>\n",
       "      <th>...</th>\n",
       "      <th>시기</th>\n",
       "      <th>신호</th>\n",
       "      <th>운영</th>\n",
       "      <th>다루</th>\n",
       "      <th>갑자기</th>\n",
       "      <th>딸</th>\n",
       "      <th>신체</th>\n",
       "      <th>미국인</th>\n",
       "      <th>만일</th>\n",
       "      <th>생명체</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>고</th>\n",
       "      <td>0</td>\n",
       "      <td>1034</td>\n",
       "      <td>165</td>\n",
       "      <td>7</td>\n",
       "      <td>1788</td>\n",
       "      <td>8</td>\n",
       "      <td>839</td>\n",
       "      <td>1288</td>\n",
       "      <td>924</td>\n",
       "      <td>3204</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>127</td>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>것</th>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>5473</td>\n",
       "      <td>13</td>\n",
       "      <td>282</td>\n",
       "      <td>15019</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>563</td>\n",
       "      <td>1109</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>한</th>\n",
       "      <td>1670</td>\n",
       "      <td>5202</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>529</td>\n",
       "      <td>57</td>\n",
       "      <td>267</td>\n",
       "      <td>54</td>\n",
       "      <td>1252</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>죠</th>\n",
       "      <td>1307</td>\n",
       "      <td>2636</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>1391</td>\n",
       "      <td>96</td>\n",
       "      <td>5316</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>그</th>\n",
       "      <td>2939</td>\n",
       "      <td>568</td>\n",
       "      <td>219</td>\n",
       "      <td>827</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>1045</td>\n",
       "      <td>46</td>\n",
       "      <td>628</td>\n",
       "      <td>517</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      고     것     한    죠     그    입니다    우리     수    에서     었  ...  시기  신호  \\\n",
       "고     0  1034   165    7  1788      8   839  1288   924  3204  ...   6   8   \n",
       "것   364     0  5473   13   282  15019    36     4   563  1109  ...   5   3   \n",
       "한  1670  5202     0  104   529     57   267    54  1252    41  ...  75  27   \n",
       "죠  1307  2636   363    0   153      2   107  1391    96  5316  ...  20  12   \n",
       "그  2939   568   219  827     0    578  1045    46   628   517  ...  34  19   \n",
       "\n",
       "    운영   다루  갑자기   딸  신체  미국인  만일  생명체  \n",
       "고  127  103   17  10   5   11   1    5  \n",
       "것    6   37   11   2   1    0   2    3  \n",
       "한    5    2   20   4  20   16   8   32  \n",
       "죠    7   12    1   0   0    2   3    3  \n",
       "그    1    0   28   6   5    5  28    7  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 앞에서 작성한 동시발생 정보를 활용하여 벡터 데이터를 생성합니다\n",
    "def co_occurrence(co_dict, vocab):\n",
    "    data = []\n",
    "    for word1 in vocab:\n",
    "        row = []\n",
    "        for word2 in vocab:\n",
    "            try: count = co_dict[(word1, word2)]\n",
    "            except KeyError: count = 0\n",
    "            row.append(count)   \n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data, index=vocab, columns=vocab)\n",
    "      \n",
    "import torch\n",
    "co = co_occurrence(co_dict, tfs.index[:1000])\n",
    "torch.save(co, 'co.pth')  # 결과를 co.pth 로 저장 합니다\n",
    "co.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 유사도 측정**\n",
    "$$\n",
    "\\text{d}_{\\text{L1}}(w,v)=\\sum_{i=1}^d{|w_i-v_i|},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 distance:\n",
      "우리 (0.0), 저 (24246.0), 제 (26486.0), 여러분 (30102.0), 그 (33590.0), 그것 (34206.0), 이런 (34273.0), 이것 (34569.0), 그리고 (34732.0), 어떤 (35048.0), 나 (35285.0), ' (35615.0), -- (35832.0), 요 (35935.0), 그런 (36570.0), 당신 (36757.0), 바로 (36803.0), 여기 (36804.0), 하지만 (36811.0), 그래서 (36900.0), 어떻게 (36977.0), 다 (37061.0), 저희 (37277.0), 모든 (37366.0), 살 (37483.0), 미국 (37631.0), 새로운 (37684.0), 다른 (37693.0), 사실 (37719.0), 까지 (37754.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def get_nearest(query, dataframe, metric, top_k, ascending=True):\n",
    "    vector = torch.FloatTensor(dataframe.loc[query].values)\n",
    "    distances = dataframe.apply(lambda x: metric(vector, torch.FloatTensor(x.values)), axis=1)\n",
    "    top_distances = distances.sort_values(ascending=ascending)[:top_k]\n",
    "    print(', '.join([f'{k} ({v:.1f})' for k, v in top_distances.items()]))\n",
    "\n",
    "# \"우리\" 단어와 l1 거리측정 함수를 활용한 계산  \n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "\n",
    "print('L1 distance:')\n",
    "get_nearest('우리', co, get_l1_distance, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{d}_{\\text{L2}}(w,v)=\\sqrt{\\sum_{i=1}^d{(w_i-v_i)^2}},\\text{ where }w,v\\in\\mathbb{R}^d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 distance:\n",
      "우리 (0.0), 저 (24246.0), 제 (26486.0), 여러분 (30102.0), 그 (33590.0), 그것 (34206.0), 이런 (34273.0), 이것 (34569.0), 그리고 (34732.0), 어떤 (35048.0), 나 (35285.0), ' (35615.0), -- (35832.0), 요 (35935.0), 그런 (36570.0), 당신 (36757.0), 바로 (36803.0), 여기 (36804.0), 하지만 (36811.0), 그래서 (36900.0), 어떻게 (36977.0), 다 (37061.0), 저희 (37277.0), 모든 (37366.0), 살 (37483.0), 미국 (37631.0), 새로운 (37684.0), 다른 (37693.0), 사실 (37719.0), 까지 (37754.0)\n"
     ]
    }
   ],
   "source": [
    "# l2 거리측정 함수\n",
    "def get_l1_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).sum()\n",
    "\n",
    "print('L1 distance:')\n",
    "get_nearest('우리', co, get_l1_distance, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d_{\\infty}(w,v)=\\max(|w_1-v_1|,|w_2-v_2|,\\cdots,|w_d-v_d|),\\text{ where }w,v\\in\\mathbb{R}^d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Infinity distance:\n",
      "우리 (0.0), 저 (852.0), 여러분 (1031.0), 자신 (1309.0), 모두 (1316.0), 나 (1376.0), 물 (1396.0), 당신 (1396.0), 영향 (1424.0), 스스로 (1434.0), 그녀 (1447.0), 필요 (1456.0), 어떤 (1459.0), 이런 (1465.0), 서로 (1481.0), 아이 (1483.0), 그 (1485.0), 이렇게 (1487.0), 만 (1487.0), 가장 (1499.0), 누구 (1501.0), 저희 (1501.0), 이야기 (1506.0), 로 (1506.0), 도움 (1511.0), 큰 (1512.0), 매우 (1517.0), 일 (1518.0), 질문 (1519.0), 보여 (1521.0)\n"
     ]
    }
   ],
   "source": [
    "def get_infinity_distance(x1, x2):\n",
    "    return ((x1 - x2).abs()).max()\n",
    "\n",
    "print('\\nInfinity distance:')\n",
    "get_nearest('우리', co, get_infinity_distance, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}_{\\text{cos}}(w,v)&=\\overbrace{\\frac{w\\cdot v}{|w||v|}}^{\\text{dot product}}\n",
    "=\\overbrace{\\frac{w}{|w|}}^{\\text{unit vector}}\\cdot\\frac{v}{|v|} \\\\\n",
    "&=\\frac{\\sum_{i=1}^{d}{w_iv_i}}{\\sqrt{\\sum_{i=1}^d{w_i^2}}\\sqrt{\\sum_{i=1}^d{v_i^2}}} \\\\\n",
    "\\text{where }&w,v\\in\\mathbb{R}^d\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarity:\n",
      "우리 (1.0), 저희 (0.9), 저 (0.8), 그녀 (0.8), 그 (0.8), 여러분 (0.7), 제 (0.7), 당신 (0.7), 누군가 (0.7), 그것 (0.7), 우린 (0.7), 새 (0.7), 일종 (0.7), 이것 (0.7), 환자 (0.7), 하루 (0.7), 그냥 (0.6), 마치 (0.6), 전화 (0.6), 각각 (0.6), 인터넷 (0.6), 컴퓨터 (0.6), 수많 (0.6), 뭔가 (0.6), 빛 (0.6), 누가 (0.6), 로봇 (0.6), 약간 (0.6), 물론 (0.6), 나무 (0.6)\n"
     ]
    }
   ],
   "source": [
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)\n",
    "\n",
    "print('\\nCosine similarity:')\n",
    "get_nearest('우리', co, get_cosine_similarity, 30, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{sim}_{\\text{jaccard}}(w,v)&=\\frac{|w \\cap v|}{|w \\cup v|} \\\\\n",
    "&=\\frac{|w \\cap v|}{|w|+|v|-|w \\cap v|} \\\\\n",
    "&\\approx\\frac{\\sum_{i=1}^d{\\min(w_i,v_i)}}{\\sum_{i=1}^d{\\max(w_i,v_i)}} \\\\\n",
    "\\text{where }&w,v\\in\\mathbb{R}^d.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard similarity:\n",
      "우리 (1.0), 그 (0.5), 저 (0.5), 제 (0.4), 여러분 (0.3), 말 (0.3), \" (0.3), 그리고 (0.3), 사람 (0.3), 일 (0.3), 도 (0.2), 다른 (0.2), 나 (0.2), 이런 (0.2), 한 (0.2), 만 (0.2), 모든 (0.2), 어떤 (0.2), 더 (0.2), 와 (0.2), 에서 (0.2), 과 (0.2), 로 (0.2), 보 (0.2), 요 (0.2), 된 (0.2), ' (0.2), 그것 (0.2), 적 (0.2), 되 (0.2)\n"
     ]
    }
   ],
   "source": [
    "def get_jaccard_similarity(x1, x2):\n",
    "    return torch.stack([x1, x2]).min(dim=0)[0].sum() / torch.stack([x1, x2]).max(dim=0)[0].sum()\n",
    "\n",
    "print('\\nJaccard similarity:')\n",
    "get_nearest('우리', co, get_jaccard_similarity, 30, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 Lesk Algorithm : 시소러스(WordNet) 기반의 중의성(WSD) 해소**\n",
    "단어의 중의성 **(WSD: Word Sense Disambiguation)** 을 알고리즘으로 극복가능\n",
    "1. **wordnet 내부** 설명과, **분석대상 문장** 의 단어 유사도를 측정합니다.\n",
    "1. 이를 활용하여 어떤 문장과 연관성이 높은지를 식별 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Synset('sea_bass.n.01'), 'the lean flesh of a saltwater fish of the family Serranidae')\n",
      "(Synset('bass.n.02'), 'the lowest part in polyphonic music')\n",
      "(Synset('sea_bass.n.01'), 'the lean flesh of a saltwater fish of the family Serranidae')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "def lesk_token(sentence, word):\n",
    "    sentence = sentence.split()\n",
    "    best_synset = lesk(sentence, word)\n",
    "    return best_synset, best_synset.definition()\n",
    "\n",
    "sentence1 = 'I went fishing last weekend and I got a bass and cooked it'\n",
    "sentence2 = 'I love the music from the speaker which has strong beat and bass'\n",
    "sentence3 = 'I think the bass is more important than guitar'\n",
    "\n",
    "word = 'bass'\n",
    "for _ in [sentence1, sentence2, sentence3]:\n",
    "    print(lesk_token(str(_), word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 Philip Resnik 선택 선호도: 중의성(WSD) 해소**\n",
    "1. 문장 내 출현한 단어는 **주변 단어들에 따라 의미가 결정** 됩니다\n",
    "1. 주변 단어들의 **품사(분포) 차이가 크면, 해당 술어가 강한 선택 선호도를** 갖습니다.\n",
    "1. 이를 공식화 하여 알고리름을 정의 합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 Ketrin Erk 단어간 유사도 측정**\n",
    "Similarity Based Method [Erk et al.2007]\n",
    "1. **데이터를 기반** 으로, 별도의 시소러스 없이도 단어간 유사도를 측정\n",
    "1. $W$ 술어와 $h$ 표제어, 그리고 두 단어 사이의 관계인 $R$ 튜플(tuple) 로 주어집니다\n",
    "1. 이러한 계산시 문제점은 0 희소벡터의 값들이 많이 발생하는 것입니다.\n",
    "1. 희소벡터를 줄이기 위해 임베팅 기법으로 **Dense 벡터** 를 만들어서 극복 합니다\n",
    "\n",
    "$$\n",
    "(w,h,R),\\text{ where }R\\text{ is a relationship, such as verb-object}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_R(w,h_0)=\\sum_{h\\in\\text{Seen}_R(w)}{\\text{sim}(h_0,h)\\cdot \\phi_R(w,h)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 s, sys: 128 ms, total: 47.4 s\n",
      "Wall time: 47.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['사실', '촌충', '코', '시장', '소유자', '아내', '결국', '미래', '거론', '경우']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 데이터 불러오기\n",
    "with open('../backup/ted.aligned.ko.refined.tok.txt') as f:\n",
    "    lines = [l.strip() for l in f.read().splitlines() if l.strip()]\n",
    "\n",
    "# Seen(w) 함수의 계산 : 문장 내 서술어(\"VV\") 와 표제어(\"NNG\") 를 찾는다 \n",
    "from konlpy.tag import Mecab\n",
    "def count_seen_headwords(lines, predicate='VV', headword='NNG'):\n",
    "    tagger, seen_dict  = Mecab(), {}    \n",
    "    for line in lines:\n",
    "        pos_result     = tagger.pos(line)\n",
    "        word_h, word_p = None, None\n",
    "        for word, pos in pos_result:\n",
    "            if pos == predicate or pos[:3] == predicate + '+':\n",
    "                word_p = word; break\n",
    "            if pos == headword:\n",
    "                word_h = word\n",
    "        \n",
    "        if word_h is not None and word_p is not None:\n",
    "            seen_dict[word_p] = [word_h] +\\\n",
    "                ([] if seen_dict.get(word_p) is None else seen_dict[word_p])            \n",
    "    return seen_dict\n",
    "\n",
    "seen_headwords = count_seen_headwords(lines)\n",
    "list(seen_headwords.keys())[:10] # key 값들 살펴보기\n",
    "seen_headwords[list(seen_headwords.keys())[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 Seen() 데이터를 바탕으로 선택관련도 점수를 계산 합니다\n",
    "import torch\n",
    "def get_selectional_association(predicate, headword, lines, dataframe, metric):\n",
    "    v1    = torch.FloatTensor(dataframe.loc[headword].values)\n",
    "    seens = seen_headwords[predicate]\n",
    "    total = 0\n",
    "    for seen in seens:\n",
    "        try:\n",
    "            v2     = torch.FloatTensor(dataframe.loc[seen].values)\n",
    "            total += metric(v1, v2)\n",
    "        except: pass\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(3265.9070), tensor(3522.6216), tensor(3642.2668)]\n"
     ]
    }
   ],
   "source": [
    "co = torch.load('co.pth')\n",
    "\n",
    "# 계산 결과를 활용하여 주어진 술어에 대해 올바른 headword 를 찾는 wsd 함수\n",
    "def get_cosine_similarity(x1, x2):\n",
    "    return (x1 * x2).sum() / ((x1**2).sum()**.5 * (x2**2).sum()**.5)\n",
    "\n",
    "def wsd(predicate, headwords):\n",
    "    selectional_associations = []\n",
    "    for h in headwords:\n",
    "        selectional_associations += [get_selectional_association(\n",
    "            predicate, h, lines, co, get_cosine_similarity)]\n",
    "    print(selectional_associations)\n",
    "    \n",
    "wsd('가', ['학교', '사람', '질문'])\n",
    "# wsd('피우', ['담배', '맥주', '사과'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
